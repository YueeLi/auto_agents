import uuid
import json
import asyncio
from datetime import datetime
from typing import Annotated, List, Literal, Optional, TypedDict, Any, Dict, Union
from dataclasses import dataclass, field
from enum import Enum

from src.graph.utils import build_image

# LangGraph imports
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from langgraph.checkpoint.memory import MemorySaver
from langgraph.types import interrupt
from langgraph.prebuilt import ToolNode, tools_condition
from langgraph.types import Send

# LangChain imports
from langchain_core.messages import BaseMessage, AIMessage, HumanMessage, SystemMessage, ToolMessage
from langchain_core.tools import tool
from langchain_core.runnables import RunnableLambda
from langchain_core.callbacks import CallbackManager

# Utility imports
import logging
from pathlib import Path

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# --- Enums and Data Classes ---

class ProcessingStatus(Enum):
    PENDING = "pending"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"
    NEEDS_HUMAN_REVIEW = "needs_human_review"

class ValidationResult(Enum):
    PASS = "pass"
    FAIL = "fail"
    NEEDS_RETRY = "needs_retry"

@dataclass
class DocumentMetadata:
    file_type: str
    file_size: int
    pages: int
    language: str
    confidence: float = 0.0
    creation_date: Optional[datetime] = None

@dataclass
class ProcessingMetrics:
    start_time: datetime = field(default_factory=datetime.now)
    end_time: Optional[datetime] = None
    retry_count: int = 0
    validation_steps: int = 0
    human_interventions: int = 0

@dataclass
class MemoryItem:
    key: str
    value: Any
    timestamp: datetime = field(default_factory=datetime.now)
    ttl: Optional[int] = None  # Time to live in seconds

# --- Complex State Definition ---

class ComprehensiveState(TypedDict):
    """Comprehensive state covering all LangGraph features."""
    
    # Core message handling
    messages: Annotated[List[BaseMessage], add_messages]
    
    # Document processing
    document_content: Optional[str]
    document_metadata: Optional[DocumentMetadata]
    extracted_text: Optional[str]
    generated_summary: Optional[str]
    processing_status: ProcessingStatus
    
    # Human-in-the-loop
    human_approval_status: Optional[Literal["approved", "rejected", "pending"]]
    human_feedback: Optional[str]
    requires_human_review: bool
    
    # Validation and quality control
    validation_result: Optional[ValidationResult]
    validation_count: int
    quality_score: float
    
    # Error handling and retry logic
    error_messages: List[str]
    retry_count: int
    max_retries: int
    
    # Memory and persistence
    short_term_memory: Dict[str, Any]
    long_term_memory: List[MemoryItem]
    session_context: Dict[str, Any]
    
    # Streaming and real-time feedback
    streaming_enabled: bool
    real_time_updates: List[str]
    
    # Processing metrics
    metrics: ProcessingMetrics
    
    # Parallel processing results
    parallel_results: Dict[str, Any]
    
    # Tool usage tracking
    tools_used: List[str]
    tool_results: Dict[str, Any]
    
    # Subgraph communication
    subgraph_states: Dict[str, Any]
    
    # Configuration
    config: Dict[str, Any]

# --- Tool Definitions ---

@tool
def ocr_text_extraction(content: str) -> Dict[str, Any]:
    """Simulate OCR text extraction from document content."""
    logger.info("üîç Executing OCR text extraction")
    # Simulate OCR processing
    extracted_text = f"OCR_EXTRACTED: {content[:100]}..."
    confidence = 0.95
    return {
        "extracted_text": extracted_text,
        "confidence": confidence,
        "method": "OCR",
        "timestamp": datetime.now().isoformat()
    }

@tool
def metadata_analyzer(content: str) -> Dict[str, Any]:
    """Analyze document metadata."""
    logger.info("üìä Analyzing document metadata")
    return {
        "file_type": "pdf",
        "estimated_pages": len(content) // 500,
        "language": "english",
        "complexity": "medium",
        "timestamp": datetime.now().isoformat()
    }

@tool
def llm_summarizer(text: str) -> Dict[str, Any]:
    """Generate summary using LLM."""
    logger.info("üìù Generating LLM summary")
    summary = f"SUMMARY: {text[:50]}... [Generated by LLM]"
    return {
        "summary": summary,
        "key_points": ["Point 1", "Point 2", "Point 3"],
        "word_count": len(text.split()),
        "timestamp": datetime.now().isoformat()
    }

@tool
def quality_validator(data: Dict[str, Any]) -> Dict[str, Any]:
    """Validate processing quality."""
    logger.info("‚úÖ Running quality validation")
    score = 0.85  # Simulated quality score
    return {
        "quality_score": score,
        "validation_passed": score > 0.8,
        "issues": [] if score > 0.8 else ["Low confidence in OCR"],
        "timestamp": datetime.now().isoformat()
    }

# Create tool node
tools = [ocr_text_extraction, metadata_analyzer, llm_summarizer, quality_validator]
tool_node = ToolNode(tools)

# --- Subgraph Definitions ---

class SubgraphState(TypedDict):
    """State for subgraphs."""
    messages: Annotated[List[BaseMessage], add_messages]
    input_data: Any
    output_data: Any
    processing_type: str

def create_text_extraction_subgraph():
    """Create text extraction subgraph."""
    
    def extract_text_node(state: SubgraphState) -> Dict[str, Any]:
        logger.info("üìÑ Text Extraction Subgraph - Processing")
        input_data = state.get("input_data", "")
        # Simulate text extraction
        extracted = f"EXTRACTED_TEXT: {input_data}"
        return {
            "output_data": extracted,
            "messages": [AIMessage(content="Text extraction completed in subgraph")]
        }
    
    def validate_extraction(state: SubgraphState) -> Dict[str, Any]:
        logger.info("üîç Text Extraction Subgraph - Validation")
        return {
            "messages": [AIMessage(content="Text extraction validated")]
        }
    
    # Build subgraph
    subgraph = StateGraph(SubgraphState)
    subgraph.add_node("extract", extract_text_node)
    subgraph.add_node("validate", validate_extraction)
    subgraph.set_entry_point("extract")
    subgraph.add_edge("extract", "validate")
    subgraph.add_edge("validate", END)
    graph = subgraph.compile(name="TextExtractionSubgraph")
    build_image(graph)
    return graph

def create_metadata_extraction_subgraph():
    """Create metadata extraction subgraph."""
    
    def extract_metadata_node(state: SubgraphState) -> Dict[str, Any]:
        logger.info("üìä Metadata Extraction Subgraph - Processing")
        input_data = state.get("input_data", "")
        metadata = {
            "file_type": "document",
            "size": len(str(input_data)),
            "complexity": "medium"
        }
        return {
            "output_data": metadata,
            "messages": [AIMessage(content="Metadata extraction completed")]
        }
    
    subgraph = StateGraph(SubgraphState)
    subgraph.add_node("extract_metadata", extract_metadata_node)
    subgraph.set_entry_point("extract_metadata")
    subgraph.add_edge("extract_metadata", END)
    graph = subgraph.compile(name="MetadataExtractionSubgraph")
    build_image(graph)
    return graph

def create_summary_generation_subgraph():
    """Create summary generation subgraph."""
    
    def generate_summary_node(state: SubgraphState) -> Dict[str, Any]:
        logger.info("üìù Summary Generation Subgraph - Processing")
        input_data = state.get("input_data", "")
        summary = f"SUMMARY: {str(input_data)[:100]}..."
        return {
            "output_data": summary,
            "messages": [AIMessage(content="Summary generation completed")]
        }
    
    subgraph = StateGraph(SubgraphState)
    subgraph.add_node("generate_summary", generate_summary_node)
    subgraph.set_entry_point("generate_summary")
    subgraph.add_edge("generate_summary", END)
    graph = subgraph.compile(name="SummaryGenerationSubgraph")
    build_image(graph)
    return graph

# --- Custom Callback Handler for Streaming ---

class StreamingCallbackHandler(CallbackManager):
    """Custom callback handler for streaming updates."""
    
    def __init__(self):
        self.updates = []
    
    def on_chain_start(self, serialized, inputs, **kwargs):
        self.updates.append(f"üöÄ Started: {serialized.get('name', 'Unknown')}")
    
    def on_chain_end(self, outputs, **kwargs):
        self.updates.append("‚úÖ Chain completed")
    
    def on_tool_start(self, serialized, input_str, **kwargs):
        self.updates.append(f"üîß Tool started: {serialized.get('name', 'Unknown')}")

# --- Node Implementations ---

def document_input_node(state: ComprehensiveState) -> Dict[str, Any]:
    """Handle document input and initial processing."""
    logger.info("üì• Document Input Node - Processing")
    
    # Simulate document input
    if not state.get("document_content"):
        content = "This is a sample document content for comprehensive LangGraph testing."
    else:
        content = state["document_content"]
    
    # Update metrics
    metrics = state.get("metrics", ProcessingMetrics())
    metrics.start_time = datetime.now()
    
    return {
        "document_content": content,
        "processing_status": ProcessingStatus.PROCESSING,
        "messages": [AIMessage(content="Document received and initial processing started")],
        "metrics": metrics,
        "short_term_memory": {"last_document": content[:50]},
        "real_time_updates": ["Document input received"]
    }

def preprocessing_node(state: ComprehensiveState) -> Dict[str, Any]:
    """Preprocess the document."""
    logger.info("‚öôÔ∏è Preprocessing Node - Processing")
    
    content = state.get("document_content", "")
    
    # Basic preprocessing
    processed_content = content.strip().replace("\n\n", "\n")
    
    # Update memory
    short_term_memory = state.get("short_term_memory", {})
    short_term_memory["preprocessing_done"] = True
    
    return {
        "document_content": processed_content,
        "messages": [AIMessage(content="Document preprocessing completed")],
        "short_term_memory": short_term_memory,
        "real_time_updates": state.get("real_time_updates", []) + ["Preprocessing completed"]
    }

def quality_check_node(state: ComprehensiveState) -> Dict[str, Any]:
    """Perform quality check on the document."""
    logger.info("üîç Quality Check Node - Processing")
    
    content = state.get("document_content", "")
    
    # Simulate quality check
    quality_score = 0.9 if len(content) > 50 else 0.6
    
    # Update validation count
    validation_count = state.get("validation_count", 0) + 1
    
    return {
        "quality_score": quality_score,
        "validation_count": validation_count,
        "messages": [AIMessage(content=f"Quality check completed. Score: {quality_score}")],
        "tools_used": state.get("tools_used", []) + ["quality_validator"],
        "real_time_updates": state.get("real_time_updates", []) + [f"Quality score: {quality_score}"]
    }

def parallel_branch_node(state: ComprehensiveState) -> List[Send]:
    """Create parallel branches for processing."""
    logger.info("üîÄ Parallel Branch Node - Creating parallel tasks")
    
    content = state.get("document_content", "")
    
    # Create parallel sends to subgraphs
    return [
        Send("text_extraction_subgraph", {
            "input_data": content,
            "processing_type": "text_extraction"
        }),
        Send("metadata_extraction_subgraph", {
            "input_data": content,
            "processing_type": "metadata_extraction"
        }),
        Send("summary_generation_subgraph", {
            "input_data": content,
            "processing_type": "summary_generation"
        })
    ]

def merge_results_node(state: ComprehensiveState) -> Dict[str, Any]:
    """Merge results from parallel processing."""
    logger.info("üîÑ Merge Results Node - Combining parallel results")
    
    # Simulate merging parallel results
    parallel_results = {
        "text_extraction": "Extracted text content",
        "metadata_extraction": {"type": "document", "pages": 1},
        "summary_generation": "Generated summary"
    }
    
    return {
        "parallel_results": parallel_results,
        "messages": [AIMessage(content="Parallel processing results merged successfully")],
        "real_time_updates": state.get("real_time_updates", []) + ["Results merged"]
    }

def human_review_node(state: ComprehensiveState) -> Dict[str, Any]:
    """Handle human review interruption."""
    logger.info("üë§ Human Review Node - Requesting human intervention")
    
    # Check if human has already provided feedback
    if state.get("human_approval_status") in ["approved", "rejected"]:
        return {
            "messages": [AIMessage(content=f"Human review result: {state['human_approval_status']}")]
        }
    
    # Request human intervention
    logger.info("‚è∏Ô∏è Interrupting for human review")
    interrupt(value={
        "type": "human_review_required",
        "content": state.get("document_content", ""),
        "quality_score": state.get("quality_score", 0.0)
    })
    
    return {}

def validation_node(state: ComprehensiveState) -> Dict[str, Any]:
    """Multi-turn validation node."""
    logger.info("‚úÖ Validation Node - Performing validation")
    
    validation_count = state.get("validation_count", 0)
    max_validations = 3
    
    if validation_count < max_validations:
        logger.info(f"‚è∏Ô∏è Validation step {validation_count + 1}/{max_validations}")
        interrupt(value={
            "type": "validation_step",
            "step": validation_count + 1,
            "max_steps": max_validations
        })
        
        return {
            "validation_count": validation_count + 1,
            "messages": [AIMessage(content=f"Validation step {validation_count + 1} completed")]
        }
    
    return {
        "validation_result": ValidationResult.PASS,
        "messages": [AIMessage(content="All validation steps completed")]
    }

def memory_update_node(state: ComprehensiveState) -> Dict[str, Any]:
    """Update both short-term and long-term memory."""
    logger.info("üß† Memory Update Node - Updating memory systems")
    
    # Update short-term memory
    short_term_memory = state.get("short_term_memory", {})
    short_term_memory.update({
        "last_processed": datetime.now().isoformat(),
        "quality_score": state.get("quality_score", 0.0),
        "validation_passed": True
    })
    
    # Update long-term memory
    long_term_memory = state.get("long_term_memory", [])
    long_term_memory.append(MemoryItem(
        key="document_processed",
        value={
            "timestamp": datetime.now().isoformat(),
            "quality": state.get("quality_score", 0.0),
            "summary": state.get("generated_summary", "")
        }
    ))
    
    return {
        "short_term_memory": short_term_memory,
        "long_term_memory": long_term_memory,
        "messages": [AIMessage(content="Memory systems updated successfully")]
    }

def streaming_feedback_node(state: ComprehensiveState) -> Dict[str, Any]:
    """Provide streaming feedback."""
    logger.info("üì° Streaming Feedback Node - Providing real-time updates")
    
    if state.get("streaming_enabled", True):
        updates = [
            "üîÑ Processing in progress...",
            "üìä Analyzing document structure...",
            "‚ú® Generating insights...",
            "üéØ Finalizing results..."
        ]
        
        return {
            "real_time_updates": state.get("real_time_updates", []) + updates,
            "messages": [AIMessage(content="Streaming updates provided")]
        }
    
    return {}

def error_handler_node(state: ComprehensiveState) -> Dict[str, Any]:
    """Handle errors and implement retry logic."""
    logger.info("‚ö†Ô∏è Error Handler Node - Processing errors")
    
    error_messages = state.get("error_messages", [])
    retry_count = state.get("retry_count", 0)
    max_retries = state.get("max_retries", 3)
    
    if retry_count < max_retries:
        logger.info(f"üîÑ Retry attempt {retry_count + 1}/{max_retries}")
        return {
            "retry_count": retry_count + 1,
            "processing_status": ProcessingStatus.PROCESSING,
            "messages": [AIMessage(content=f"Retrying... Attempt {retry_count + 1}")]
        }
    else:
        logger.info("‚ùå Max retries exceeded")
        return {
            "processing_status": ProcessingStatus.FAILED,
            "messages": [AIMessage(content="Processing failed after maximum retries")]
        }

def final_output_node(state: ComprehensiveState) -> Dict[str, Any]:
    """Generate final output."""
    logger.info("üéØ Final Output Node - Generating final results")
    
    # Update metrics
    metrics = state.get("metrics", ProcessingMetrics())
    metrics.end_time = datetime.now()
    
    # Compile final results
    final_results = {
        "status": "completed",
        "quality_score": state.get("quality_score", 0.0),
        "parallel_results": state.get("parallel_results", {}),
        "memory_items": len(state.get("long_term_memory", [])),
        "tools_used": state.get("tools_used", []),
        "validation_count": state.get("validation_count", 0),
        "retry_count": state.get("retry_count", 0),
        "processing_time": (metrics.end_time - metrics.start_time).total_seconds() if metrics.end_time else 0
    }
    
    return {
        "processing_status": ProcessingStatus.COMPLETED,
        "metrics": metrics,
        "final_results": final_results,
        "messages": [AIMessage(content=f"Processing completed successfully: {json.dumps(final_results, indent=2)}")],
        "real_time_updates": state.get("real_time_updates", []) + ["‚úÖ Processing completed!"]
    }

# --- Conditional Edge Functions ---

def quality_check_router(state: ComprehensiveState) -> Literal["parallel_branch", "error_handler", "human_review"]:
    """Route based on quality check results."""
    quality_score = state.get("quality_score", 0.0)
    
    if quality_score > 0.8:
        logger.info("‚úÖ Quality check passed - routing to parallel processing")
        return "parallel_branch"
    elif quality_score > 0.5:
        logger.info("‚ö†Ô∏è Quality check requires human review")
        return "human_review"
    else:
        logger.info("‚ùå Quality check failed - routing to error handler")
        return "error_handler"

def human_approval_router(state: ComprehensiveState) -> Literal["parallel_branch", "error_handler"]:
    """Route based on human approval."""
    approval_status = state.get("human_approval_status")
    
    if approval_status == "approved":
        logger.info("‚úÖ Human approved - continuing processing")
        return "parallel_branch"
    else:
        logger.info("‚ùå Human rejected - routing to error handler")
        return "error_handler"

def validation_router(state: ComprehensiveState) -> Literal["memory_update", "error_handler", "validation"]:
    """Route based on validation results."""
    validation_count = state.get("validation_count", 0)
    validation_result = state.get("validation_result")
    
    if validation_result == ValidationResult.PASS:
        logger.info("‚úÖ Validation passed - updating memory")
        return "memory_update"
    elif validation_count >= 3:
        logger.info("‚ùå Max validation attempts reached")
        return "error_handler"
    else:
        logger.info("üîÑ Continuing validation")
        return "validation"

def should_stream_feedback(state: ComprehensiveState) -> Literal["streaming_feedback", "final_output"]:
    """Decide whether to provide streaming feedback."""
    if state.get("streaming_enabled", True):
        return "streaming_feedback"
    return "final_output"

# --- Main Graph Construction ---

def create_comprehensive_graph():
    """Create the comprehensive LangGraph system."""
    
    # Initialize subgraphs
    text_extraction_subgraph = create_text_extraction_subgraph()
    metadata_extraction_subgraph = create_metadata_extraction_subgraph()
    summary_generation_subgraph = create_summary_generation_subgraph()
    
    # Create main graph
    graph_builder = StateGraph(ComprehensiveState)
    
    # Add all nodes
    graph_builder.add_node("document_input", document_input_node)
    graph_builder.add_node("preprocessing", preprocessing_node)
    graph_builder.add_node("quality_check", quality_check_node)
    graph_builder.add_node("parallel_branch", parallel_branch_node)
    graph_builder.add_node("text_extraction_subgraph", text_extraction_subgraph)
    graph_builder.add_node("metadata_extraction_subgraph", metadata_extraction_subgraph)
    graph_builder.add_node("summary_generation_subgraph", summary_generation_subgraph)
    graph_builder.add_node("merge_results", merge_results_node)
    graph_builder.add_node("human_review", human_review_node)
    graph_builder.add_node("validation", validation_node)
    graph_builder.add_node("memory_update", memory_update_node)
    graph_builder.add_node("streaming_feedback", streaming_feedback_node)
    graph_builder.add_node("error_handler", error_handler_node)
    graph_builder.add_node("final_output", final_output_node)
    graph_builder.add_node("tools", tool_node)
    
    # Set entry point
    graph_builder.set_entry_point("document_input")
    
    # Add edges
    graph_builder.add_edge("document_input", "preprocessing")
    graph_builder.add_edge("preprocessing", "quality_check")
    
    # Add conditional edges
    graph_builder.add_conditional_edges(
        "quality_check",
        quality_check_router,
        {
            "parallel_branch": "parallel_branch",
            "error_handler": "error_handler",
            "human_review": "human_review"
        }
    )
    
    graph_builder.add_conditional_edges(
        "human_review",
        human_approval_router,
        {
            "parallel_branch": "parallel_branch",
            "error_handler": "error_handler"
        }
    )
    
    # Parallel processing edges
    graph_builder.add_edge("text_extraction_subgraph", "merge_results")
    graph_builder.add_edge("metadata_extraction_subgraph", "merge_results")
    graph_builder.add_edge("summary_generation_subgraph", "merge_results")
    
    graph_builder.add_edge("merge_results", "validation")
    
    graph_builder.add_conditional_edges(
        "validation",
        validation_router,
        {
            "memory_update": "memory_update",
            "error_handler": "error_handler", 
            "validation": "validation"
        }
    )
    
    graph_builder.add_conditional_edges(
        "memory_update",
        should_stream_feedback,
        {
            "streaming_feedback": "streaming_feedback",
            "final_output": "final_output"
        }
    )
    
    graph_builder.add_edge("streaming_feedback", "final_output")
    graph_builder.add_edge("error_handler", "final_output")
    graph_builder.add_edge("final_output", END)
    
    # Add tool condition for dynamic tool usage
    graph_builder.add_conditional_edges("tools", tools_condition)
    
    graph = graph_builder.compile(
        checkpointer=MemorySaver(),
        name="ComprehensiveLangGraphSystem"
    )
    build_image(graph)
    return graph

# --- Execution and Testing Framework ---

class GraphExecutionManager:
    """Manager for executing and testing the comprehensive graph."""
    
    def __init__(self):
        self.graph = create_comprehensive_graph()
        self.callback_handler = StreamingCallbackHandler()
    
    def create_initial_state(self, document_content: str = None, config: Dict[str, Any] = None) -> Dict[str, Any]:
        """Create initial state for graph execution."""
        return {
            "document_content": document_content or "Sample document for comprehensive LangGraph testing with multiple features and complex processing requirements.",
            "processing_status": ProcessingStatus.PENDING,
            "human_approval_status": None,
            "validation_count": 0,
            "retry_count": 0,
            "max_retries": 3,
            "quality_score": 0.0,
            "short_term_memory": {},
            "long_term_memory": [],
            "session_context": {},
            "streaming_enabled": True,
            "real_time_updates": [],
            "tools_used": [],
            "tool_results": {},
            "parallel_results": {},
            "subgraph_states": {},
            "error_messages": [],
            "requires_human_review": False,
            "metrics": ProcessingMetrics(),
            "config": config or {"verbose": True, "max_parallel": 3}
        }
    
    def execute_full_workflow(self, initial_state: Dict[str, Any] = None) -> Dict[str, Any]:
        """Execute the complete workflow with all interruptions."""
        thread_id = str(uuid.uuid4())
        config = {"configurable": {"thread_id": thread_id}}
        
        if initial_state is None:
            initial_state = self.create_initial_state()
        
        logger.info("üöÄ Starting Comprehensive LangGraph Execution")
        logger.info("=" * 60)
        
        try:
            # Step 1: Initial execution until first interrupt
            logger.info("üìã STEP 1: Initial execution")
            current_state = self.graph.invoke(initial_state, config)
            self._log_current_state(current_state, "After initial execution")
            
            # Step 2: Handle quality check (simulate high quality)
            logger.info("üìã STEP 2: Handling quality check")
            current_state = self.graph.invoke({}, config)
            self._log_current_state(current_state, "After quality check")
            
            # Step 3: Handle validation steps
            logger.info("üìã STEP 3: Multi-turn validation")
            for i in range(3):
                logger.info(f"   Validation step {i+1}")
                current_state = self.graph.invoke({}, config)
                self._log_current_state(current_state, f"After validation step {i+1}")
            
            # Step 4: Final completion
            logger.info("üìã STEP 4: Final completion")
            final_state = self.graph.invoke({}, config)
            self._log_current_state(final_state, "Final state")
            
            return final_state
            
        except Exception as e:
            logger.error(f"‚ùå Execution failed: {str(e)}")
            return {"error": str(e)}
    
    def execute_with_human_intervention(self) -> Dict[str, Any]:
        """Execute workflow with human intervention scenario."""
        thread_id = str(uuid.uuid4())
        config = {"configurable": {"thread_id": thread_id}}
        
        # Start with low quality to trigger human review
        initial_state = self.create_initial_state()
        initial_state["quality_score"] = 0.6  # This will trigger human review
        
        logger.info("üöÄ Starting Human Intervention Scenario")
        logger.info("=" * 60)
        
        try:
            # Step 1: Initial execution (will trigger human review)
            logger.info("üìã STEP 1: Initial execution (low quality)")
            current_state = self.graph.invoke(initial_state, config)
            
            # Step 2: Provide human approval
            logger.info("üìã STEP 2: Providing human approval")
            current_state = self.graph.invoke({
                "human_approval_status": "approved",
                "human_feedback": "Approved after manual review"
            }, config)
            
            # Step 3: Continue with validation
            logger.info("üìã STEP 3: Continuing validation")
            for i in range(3):
                current_state = self.graph.invoke({}, config)
                
            return current_state
            
        except Exception as e:
            logger.error(f"‚ùå Human intervention execution failed: {str(e)}")
            return {"error": str(e)}
    
    def execute_error_scenario(self) -> Dict[str, Any]:
        """Execute workflow with error handling scenario."""
        thread_id = str(uuid.uuid4())
        config = {"configurable": {"thread_id": thread_id}}
        
        # Start with very low quality to trigger error handling
        initial_state = self.create_initial_state()
        initial_state["quality_score"] = 0.3  # This will trigger error handling
        
        logger.info("üöÄ Starting Error Handling Scenario")
        logger.info("=" * 60)
        
        try:
            current_state = self.graph.invoke(initial_state, config)
            return current_state
            
        except Exception as e:
            logger.error(f"‚ùå Error scenario execution failed: {str(e)}")
            return {"error": str(e)}
    
    def _log_current_state(self, state: Dict[str, Any], context: str):
        """Log current state information."""
        logger.info(f"üìä {context}")
        if state:
            logger.info(f"   Status: {state.get('processing_status', 'Unknown')}")
            logger.info(f"   Quality Score: {state.get('quality_score', 0.0)}")
            logger.info(f"   Validation Count: {state.get('validation_count', 0)}")
            logger.info(f"   Messages: {len(state.get('messages', []))}")
            logger.info(f"   Updates: {len(state.get('real_time_updates', []))}")
            
            # Log recent updates
            updates = state.get('real_time_updates', [])
            if updates:
                logger.info(f"   Recent Updates: {updates[-3:]}")
        logger.info("-" * 40)

# --- Advanced Features Demo ---

def demonstrate_advanced_features():
    """Demonstrate advanced LangGraph features."""
    
    logger.info("üéØ Demonstrating Advanced LangGraph Features")
    logger.info("=" * 60)
    
    manager = GraphExecutionManager()
    
    # Feature 1: Complete workflow execution
    logger.info("üîÑ Feature 1: Complete Workflow Execution")
    result1 = manager.execute_full_workflow()
    
    # Feature 2: Human intervention workflow
    logger.info("\nüë§ Feature 2: Human Intervention Workflow")
    result2 = manager.execute_with_human_intervention()
    
    # Feature 3: Error handling workflow
    logger.info("\n‚ö†Ô∏è Feature 3: Error Handling Workflow")
    result3 = manager.execute_error_scenario()
    
    # Feature 4: Memory and persistence demonstration
    logger.info("\nüß† Feature 4: Memory and Persistence")
    demonstrate_memory_features(manager)
    
    # Feature 5: Streaming and real-time updates
    logger.info("\nüì° Feature 5: Streaming and Real-time Updates")
    demonstrate_streaming_features(manager)
    
    return {
        "complete_workflow": result1,
        "human_intervention": result2,
        "error_handling": result3
    }

def demonstrate_memory_features(manager: GraphExecutionManager):
    """Demonstrate memory and persistence features."""
    
    # Create multiple sessions to show persistence
    sessions = []
    for i in range(3):
        thread_id = str(uuid.uuid4())
        config = {"configurable": {"thread_id": thread_id}}
        
        initial_state = manager.create_initial_state(
            document_content=f"Document {i+1} for memory demonstration"
        )
        
        # Add some memory items
        initial_state["long_term_memory"] = [
            MemoryItem(
                key=f"session_{i}",
                value=f"Session {i+1} data",
                timestamp=datetime.now()
            )
        ]
        
        try:
            result = manager.graph.invoke(initial_state, config)
            sessions.append({
                "thread_id": thread_id,
                "memory_items": len(result.get("long_term_memory", [])),
                "short_term_keys": list(result.get("short_term_memory", {}).keys())
            })
        except Exception as e:
            logger.error(f"Memory demo session {i+1} failed: {str(e)}")
    
    logger.info(f"üìä Created {len(sessions)} sessions with persistent memory")
    for i, session in enumerate(sessions):
        logger.info(f"   Session {i+1}: {session}")

def demonstrate_streaming_features(manager: GraphExecutionManager):
    """Demonstrate streaming and real-time features."""
    
    initial_state = manager.create_initial_state()
    initial_state["streaming_enabled"] = True
    
    thread_id = str(uuid.uuid4())
    config = {"configurable": {"thread_id": thread_id}}
    
    try:
        # Execute with streaming enabled
        result = manager.graph.invoke(initial_state, config)
        
        updates = result.get("real_time_updates", [])
        logger.info(f"üì° Generated {len(updates)} real-time updates:")
        for i, update in enumerate(updates):
            logger.info(f"   {i+1}. {update}")
            
    except Exception as e:
        logger.error(f"Streaming demo failed: {str(e)}")

# --- Performance and Analytics ---

class GraphAnalytics:
    """Analytics and performance monitoring for the graph."""
    
    @staticmethod
    def analyze_execution_metrics(results: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze execution metrics."""
        analytics = {
            "total_executions": len([r for r in results.values() if r and not r.get("error")]),
            "failed_executions": len([r for r in results.values() if r and r.get("error")]),
            "average_processing_time": 0.0,
            "memory_usage": {},
            "tool_usage_stats": {},
            "validation_stats": {},
            "human_intervention_rate": 0.0
        }
        
        processing_times = []
        tool_usage = {}
        validation_counts = []
        human_interventions = 0
        
        for result in results.values():
            if result and not result.get("error"):
                # Processing time
                if "final_results" in result and "processing_time" in result["final_results"]:
                    processing_times.append(result["final_results"]["processing_time"])
                
                # Tool usage
                tools_used = result.get("tools_used", [])
                for tool in tools_used:
                    tool_usage[tool] = tool_usage.get(tool, 0) + 1
                
                # Validation stats
                validation_counts.append(result.get("validation_count", 0))
                
                # Human intervention
                if result.get("human_approval_status"):
                    human_interventions += 1
        
        if processing_times:
            analytics["average_processing_time"] = sum(processing_times) / len(processing_times)
        
        analytics["tool_usage_stats"] = tool_usage
        analytics["validation_stats"] = {
            "average_validations": sum(validation_counts) / len(validation_counts) if validation_counts else 0,
            "max_validations": max(validation_counts) if validation_counts else 0
        }
        analytics["human_intervention_rate"] = human_interventions / analytics["total_executions"] if analytics["total_executions"] > 0 else 0
        
        return analytics

# --- Main Execution ---

if __name__ == "__main__":
    print("üöÄ LangGraph Comprehensive System Demonstration")
    print("=" * 80)
    
    # Run comprehensive demonstration
    results = demonstrate_advanced_features()
    
    # Analyze results
    analytics = GraphAnalytics.analyze_execution_metrics(results)
    
    print("\nüìä Execution Analytics:")
    print("=" * 40)
    for key, value in analytics.items():
        print(f"{key}: {value}")
    
    print("\n‚úÖ Comprehensive LangGraph demonstration completed!")
    print("\nThis implementation covers:")
    print("‚Ä¢ Complex state management with multiple data types")
    print("‚Ä¢ Subgraph integration and parallel processing")
    print("‚Ä¢ Human-in-the-loop interactions with interrupts")
    print("‚Ä¢ Multi-turn validation and quality control")
    print("‚Ä¢ Error handling and retry mechanisms")
    print("‚Ä¢ Memory systems (short-term and long-term)")
    print("‚Ä¢ Streaming and real-time updates")
    print("‚Ä¢ Tool integration and dynamic tool usage")
    print("‚Ä¢ Performance monitoring and analytics")
    print("‚Ä¢ Conditional routing and branching logic")
    print("‚Ä¢ Persistent state with checkpointers")
    print("‚Ä¢ Custom callback handlers")
    print("‚Ä¢ Advanced type definitions and data structures")